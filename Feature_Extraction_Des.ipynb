{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"C8TGVZvwaXqX","executionInfo":{"status":"ok","timestamp":1685908072821,"user_tz":240,"elapsed":11127,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}}},"outputs":[],"source":["#!pip3 install torch torchvision torchaudio\n","#!pip3 install -U scikit-learn scipy matplotlib\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import torch.nn.functional as nnF\n","from sklearn import metrics as skl_metrics\n","from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, train_test_split\n","import pandas as pd\n","import numpy as np\n","import numpy.typing as npt\n","import csv\n","import os\n","import time\n","import copy\n","import glob\n","import json\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","\n","#!pip install grad-cam\n","#from pytorch_grad_cam import GradCAM, EigenCAM, FullGrad\n","#from pytorch_grad_cam.utils.image import show_cam_on_image\n","\n","## This allows inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware\n","import torch.backends.cudnn as cudnn\n","cudnn.benchmark = True"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0AX1N1vrac4W","executionInfo":{"status":"ok","timestamp":1685909687304,"user_tz":240,"elapsed":2052,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}},"outputId":"bffd4e95-dfcf-47c3-a65b-bb8ab0bc7cbd"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["## General parameters\n","verbose = False\n","check = False\n","\n","random_state = 42\n","\n","#toxoFolderPath = '/data/raw/Toxoplasmosis/classification'\n","toxoFolderPath = '/content/drive/MyDrive/REU/mini_eye_data'\n","\n","## Execution parameters\n","num_epochs = 100     # Epochs to be trained\n","#lr=0.001            # Learning rate for the training\n","lr=1e-3\n","#momentum=0.9        # Momentum for the training\n","returned='best'     # Model to be returned in the training. ['last' (default), 'best']\n"],"metadata":{"id":"B5FnAsdwafzG","executionInfo":{"status":"ok","timestamp":1685908096795,"user_tz":240,"elapsed":234,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    \"\"\"Class that contains a dataset.\n","\n","    Attributes\n","    ----------\n","    data : numpy.ndarray\n","        Array containing the data of the dataset\n","    targets : numpy.ndarray\n","        Array containing tha targets associated with the data\n","    \"\"\"\n","    def __init__(self, data:npt.NDArray, targets:npt.NDArray):\n","        self.data=data\n","        self.targets=targets\n","\n","    def __len__(self):\n","        \"\"\"Returns the number of data points\n","        \"\"\"\n","        return self.data.shape[0]\n","\n","    def __getitem__(self, idx:int):\n","        x = self.data[idx]\n","        y = self.targets[idx]\n","        return x, y\n","\n","class MyLazyImageDataset(Dataset):\n","    \"\"\"Class that contains an image dataset.\n","    It has lazy loading of the images, saving system memory at the cost of time.\n","\n","    Attributes\n","    ----------\n","    data : numpy.ndarray\n","        Array containing the data of the dataset\n","    targets : numpy.ndarray\n","        Array containing tha targets associated with the data\n","    transformations : torchvision.transforms\n","        Transformations to be applied to the images.\n","\n","    \"\"\"\n","    def __init__(self, data:npt.NDArray, targets:npt.NDArray, transformations:torchvision.transforms):\n","        self.data=data\n","        self.targets=targets\n","        self.transformations=transformations\n","\n","    def __len__(self):\n","        \"\"\"Returns the number of data points\n","        \"\"\"\n","        return self.data.shape[0]\n","\n","    def __getitem__(self, idx):\n","        x = self.transformations(Image.open(self.data[idx]))\n","        y = self.targets[idx]\n","        return x, y\n","\n","\n","class DataContainer:\n","    \"\"\"Class that contains a dataset. Allows data splitting plus dataset and dataloader creation\n","    Class .\n","\n","    Attributes\n","    ----------\n","    data : numpy.ndarray\n","        Array containing the data of the dataset\n","    targets : numpy.ndarray\n","        Array containing tha targets associated with the data\n","\n","    Methods\n","    -------\n","\n","    \"\"\"\n","    data = []\n","    targets = []\n","\n","    train_index = []\n","    final_val_index = []\n","    test_index = []\n","    kfold_indexes = []\n","\n","    def __init__(self, \n","                 data:npt.NDArray, \n","                 targets:npt.NDArray, \n","                 split_test:bool=True, \n","                 test_size:float=0.1, \n","                 split_final_val:bool=True, \n","                 final_val_size:float=0.1, \n","                 split_kfold:bool=True, \n","                 k_splits:int=5, \n","                 random_state:int=None):\n","        self.data = data\n","        self.targets = targets\n","        self.split_test=split_test\n","        self.test_size=test_size\n","        self.split_final_val=split_final_val\n","        self.final_val_size=final_val_size\n","        self.split_kfold=split_kfold\n","        self.k_splits = k_splits\n","        self.random_state = random_state\n","\n","        # Generate test indexes over the dataset\n","        if self.split_test:\n","            test_len=round(self.data.shape[0] * self.test_size)\n","            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_len, random_state=self.random_state)\n","            self.train_index, self.test_index = list(sss.split(self.data, self.targets))[0]\n","        else:\n","            self.train_index, self.test_index = np.array(range(0, self.data.shape[0])), []\n","        \n","        # Generate final-validation indexes over the dataset.\n","        # Intersection between taining and final-validation must be empty\n","        if self.split_final_val:\n","            # Number of examples to be separated refered to the original data size\n","            final_val_len=round(self.data.shape[0] * self.final_val_size)\n","            sss = StratifiedShuffleSplit(n_splits=1, test_size=final_val_len, random_state=self.random_state)\n","            # The final_val is seprated from the data minus test subset. \n","            train_index, final_val_index = list(sss.split(self.data[self.train_index], self.targets[self.train_index]))[0]\n","            # But the indexes obtained must be refered to the original dataset and not the training subset\n","            self.final_val_index = self.train_index[final_val_index]\n","            self.train_index = self.train_index[train_index]\n","        else:\n","            self.train_index, self.final_val_index = self.train_index, []\n","\n","        # Generate train-validation k-fold indexes over the training subset\n","        if self.split_kfold:\n","            strtfdKFold = StratifiedKFold(n_splits=self.k_splits, shuffle=True, random_state=self.random_state)\n","            self.kfold_indexes = list(strtfdKFold.split(self.data[self.train_index], self.targets[self.train_index]))\n","        else:\n","            self.kfold_indexes = []\n","    \n","    # This function is needed for the possible situation in which a lazy kind of dataset may be needed \n","    # (for instance for huge image datasets that need lazy load due to memory constraints)\n","    def getDataset_(self, data:npt.NDArray, targets:npt.NDArray):\n","        return MyDataset(data, targets)\n","    \n","\n","    def save_partition (self, file_path:str):\n","        \"\"\"Saves the partition of the dataset in the given file path\n","        \"\"\"\n","        toSave = (self.train_index, self.final_val_index, self.test_index, self.kfold_indexes)\n","        with open(file_path, 'wb') as file:\n","            pd.to_pickle(toSave, file)\n","            print(f'Object successfully saved to \"{file_path}\"')\n","\n","    def load_partition (self, file_path:str):\n","        \"\"\"Loads the partition of the dataset from the given file path\n","        \"\"\"\n","        self.train_index, self.final_val_index, self.test_index, self.kfold_indexes = pd.read_pickle(file_path)\n","\n","\n","    ############### Checks ###############\n","    \n","    def check_intersection (self, verbose:bool=False):\n","        \"\"\"Method that returns if there is any element of one subset (train, final_val, test) in the others.\n","        If verbose is True, prints tuple (test_in_train, val_in_train, val_in_test)\n","        \n","        Returns\n","        ----------\n","            True if the intersections are 0, False otherwise\n","        \"\"\"\n","        test_in_train = sum(1 if x in self.train_index else 0 for x in self.test_index)\n","        val_in_train = sum(1 if x in self.final_val_index else 0 for x in self.test_index)\n","        val_in_test = sum(1 if x in self.train_index else 0 for x in self.final_val_index)\n","        if verbose: print (test_in_train, val_in_train, val_in_test)\n","        return test_in_train + val_in_train + val_in_test == 0\n","    \n","    def check_folds (self, verbose:bool=False):\n","        \"\"\"Method that returns if the folds were correctly formed: right size AND validation not intersecting AND sum(validation) == training\n","        If verbose is True, prints tuple (size_ok, val_intersection_ok, all_val_eq_training_ok)\n","\n","        Returns\n","        ----------    \n","            True if the intersections are 0, False otherwise\n","        \"\"\"\n","        if self.kfold_indexes is None or len(self.kfold_indexes) == 0:\n","            raise FileNotFoundError('The controller was created without kfold splitting')\n","        size_ok = True\n","        val_acum = []\n","        val_intersection = 0\n","        for (train, val) in self.kfold_indexes:\n","            if train.shape[0] + val.shape[0] != self.train_index.shape[0]:\n","                size_ok = False\n","            for v in val:\n","                if v in val_acum:\n","                    val_intersection += 1\n","                else:\n","                    val_acum.append(v)\n","        val_intersection_ok = val_intersection==0\n","        val_acum_ok = len(val_acum)==self.train_index.shape[0]\n","        if verbose: print (size_ok, val_intersection_ok, val_acum_ok)\n","        return size_ok and val_intersection_ok and val_acum_ok\n","        \n","    ############### Data ###############\n","    \n","    def get_data(self):\n","        return (self.data, self.targets)\n","\n","    def get_full_train(self):\n","        return (self.data[self.train_index], self.targets[self.train_index])\n","\n","    def get_final_val(self):\n","        if self.final_val_index is None or self.final_val_index.shape[0] == 0:\n","            raise FileNotFoundError('The controller was created without final_val splitting')\n","        return (self.data[self.final_val_index], self.targets[self.final_val_index])\n","    \n","    def get_test(self):\n","        if self.test_index is None or self.test_index.shape[0] == 0:\n","            raise FileNotFoundError('The controller was created without test splitting')\n","        return (self.data[self.test_index], self.targets[self.test_index])\n","    \n","    ############### Indexes ###############\n","    \n","    def get_fold_indexes (self, fold:int) -> tuple[list[int], list[int]]:\n","        if self.kfold_indexes is None or len(self.kfold_indexes) == 0:\n","            raise FileNotFoundError('The controller was created without kfold splitting')\n","        fold_train_indexes, fold_val_indexes = self.kfold_indexes[fold]\n","        train_indexes = self.train_index[fold_train_indexes]\n","        val_indexes = self.train_index[fold_val_indexes]\n","        return (train_indexes, val_indexes)\n","\n","    ############### Dataset ###############\n","    \n","    def get_data_dataset(self) -> Dataset:\n","        data, targets = self.get_data()\n","        data_dataset = self.getDataset_(data, targets)\n","        return data_dataset\n","\n","    def get_full_train_dataset(self) -> Dataset:\n","        train_data, train_targets = self.get_full_train()\n","        return self.getDataset_(train_data, train_targets)\n","\n","    def get_final_val_dataset(self) -> Dataset:\n","        final_val_data, final_val_targets = self.get_final_val()\n","        return self.getDataset_(final_val_data, final_val_targets)\n","    \n","    def get_test_dataset(self) -> Dataset:\n","        test_data, test_targets = self.get_test()\n","        return self.getDataset_(test_data, test_targets)\n","    \n","    def get_fold_datasets (self, fold:int) -> tuple[Dataset, Dataset]:\n","        train_indexes, val_indexes = self.get_fold_indexes(fold)\n","        train_dataset = self.getDataset_(self.data[train_indexes], self.targets[train_indexes])\n","        val_dataset = self.getDataset_(self.data[val_indexes], self.targets[val_indexes])\n","        return (train_dataset, val_dataset)\n","    \n","    ############### Dataloader ###############\n","    \n","    def get_data_dataloader(self, batch_size:int=32, shuffle:bool=False, num_workers:int=2) -> DataLoader:\n","        \"\"\"Returns a dataloader with the full data.\n","\n","        Args\n","        ----------\n","        batch_size : int\n","            Size of the batch. Optional. Default 32\n","        shuffle : bool\n","            If True, the order of the examples will be random. Optional. Default False\n","        num_workers: int\n","            Number of process to use. Optional. Default 2\n","        \"\"\"\n","        data_dataset = self.get_data_dataset()\n","        return DataLoader(data_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n","\n","    def get_full_train_dataloader(self, batch_size:int=32, shuffle:bool=False, num_workers:int=2) -> DataLoader:\n","        \"\"\"Returns a dataloader with the full training subset.\n","\n","        Args\n","        ----------\n","        batch_size : int\n","            Size of the batch. Optional. Default 32\n","        shuffle : bool\n","            If True, the order of the examples will be random. Optional. Default False\n","        num_workers: int\n","            Number of process to use. Optional. Default 2\n","        \"\"\"\n","        train_dataset = self.get_full_train_dataset()\n","        return DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n","\n","    def get_final_val_dataloader(self, batch_size:int=32, shuffle:bool=False, num_workers:int=2) -> DataLoader:\n","        \"\"\"Returns a dataloader with the final validation subset.\n","\n","        Args\n","        ----------\n","        batch_size : int\n","            Size of the batch. Optional. Default 32\n","        shuffle : bool\n","            If True, the order of the examples will be random. Optional. Default False\n","        num_workers: int\n","            Number of process to use. Optional. Default 2\n","        \"\"\"\n","        final_val_dataset = self.get_final_val_dataset()\n","        return DataLoader(final_val_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n","    \n","    def get_test_dataloader(self, batch_size:int=32, shuffle:bool=True, num_workers:int=2) -> DataLoader:\n","        \"\"\"Returns a dataloader with the test subset.\n","\n","        Args\n","        ----------\n","        batch_size : int\n","            Size of the batch. Optional. Default 32\n","        shuffle : bool\n","            If True, the order of the examples will be random. Optional. Default False\n","        num_workers: int\n","            Number of process to use. Optional. Default 2\n","        \"\"\"\n","        test_dataset = self.get_test_dataset()\n","        return DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n","\n","    def get_fold_dataloaders(self, fold:int, batch_size:int=32, shuffle:bool=True, num_workers:int=2) -> tuple[DataLoader, DataLoader]:\n","        train_dataset, val_dataset = self.get_fold_datasets(fold)\n","        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n","        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n","        return (train_dataloader, val_dataloader)\n","\n","\n","class ImageFolderDataContainer(DataContainer):\n","    \"\"\"Class to process the extraction of a image dataset from a folder path.\n","    The labels are extracted from the image path as the index of pathLabelsList\n","    Extends DataContainer.\n","    Allows data splitting plus dataset and dataloader creation.\n","\n","    Attributes\n","    ----------\n","    data : numpy.ndarray\n","        Array containing the data of the dataset\n","    targets : numpy.ndarray\n","        Array containing tha targets associated with the data\n","    filepaths: numpy.ndarray[str]\n","        Array with the paths of the images\n","\n","    Methods\n","    -------\n","\n","    \"\"\"\n","    filepaths = []\n","\n","    def __init__(self, \n","                 folderPath:str,\n","                 extension:str,\n","                 pathLabelsList: list[str],\n","                 lazy:bool=False, \n","                 split_test:bool=True, \n","                 test_size:float=0.1, \n","                 split_final_val:bool=True, \n","                 final_val_size:float=0.1, \n","                 split_kfold:bool=True, \n","                 k_splits:int=5, \n","                 random_state:int=None, \n","                 transformations:bool=None):\n","        self.lazy=lazy\n","        self.transformations = transformations\n","        if self.transformations is None:\n","            self.transformations = transforms.ToTensor()\n","        data, labels = self.extract_data(folderPath, extension, pathLabelsList)\n","        super().__init__(data=data, \n","                         targets=labels, \n","                         split_test=split_test, \n","                         test_size=test_size, \n","                         split_final_val=split_final_val, \n","                         final_val_size=final_val_size, \n","                         split_kfold=split_kfold, \n","                         k_splits=k_splits, \n","                         random_state=random_state)\n","    \n","    \n","\n","    def extract_data (self, folderPath:str, extension:str, pathLabelsList: list[str]) -> tuple[npt.NDArray, npt.NDArray]:\n","        \"\"\"Function that extract the data from the given path in a recursive way.\n","        \"\"\"\n","        # Take the paths of all images for that magnification in the given folder and subfolders and short them alphabetically\n","        filepaths = glob.glob(folderPath + \"/**/*\" + extension.lower(), recursive=True)\n","        filepaths.extend(glob.glob(folderPath + \"/**/*\" + extension.upper(), recursive=True))\n","        filepaths = sorted(filepaths, key=lambda s: os.path.split(s)[-1])\n","\n","        # Read all images and extract their labels from the path\n","        images = filepaths\n","        if not self.lazy:\n","            images = [np.asarray(self.transformations(Image.open(fpath))) for fpath in images]\n","        labels=[]\n","        possibleLabelsLength = len(pathLabelsList)\n","        #print(len(filepaths))\n","        for fpath in filepaths:\n","            for i in range(0, possibleLabelsLength):\n","                if pathLabelsList[i] in fpath:\n","                    labels.append(i)\n","                    break\n","\n","        # Store filepaths, and return images and labels\n","        self.filepaths = np.asarray(filepaths)\n","        data = np.asarray(images)\n","        labels = np.asarray(labels)\n","\n","        return data, labels\n","    \n","    def getDataset_(self, data:npt.NDArray, targets:npt.NDArray) -> Dataset:\n","        \"\"\"Function intended for huge image datasets that need lazy load due to memory constraints, instanties a different Dataset class\n","\n","        Args\n","        ----------\n","        data: : numpy.ndarray\n","            Array of images or paths of images depending of if it has been created as lazy or not\n","        targets : numpy.ndarray\n","            Array of target values of the data\n","            \n","        Returns\n","        ----------\n","        Dataset:\n","            Implementation of the class torch.utils.data.Dataset. It will have lazy loading if the controller has been instantiated as such.\n","        \"\"\"\n","        if not self.lazy:\n","            return MyDataset(data, targets)\n","        else:\n","            return MyLazyImageDataset(data, targets, self.transformations)\n","    \n","    \n","    ############### Image paths ###############\n","\n","    def get_data_paths(self):\n","        return self.filepaths\n","\n","    def get_full_train_paths(self):\n","        return self.filepaths[self.train_index]\n","\n","    def get_final_val_paths(self):\n","        if self.final_val_index is None or self.final_val_index.shape[0] == 0:\n","            raise FileNotFoundError('The controller was created without final_val splitting')\n","        return self.filepaths[self.final_val_index]\n","    \n","    def get_test_paths(self):\n","        if self.test_index is None or self.test_index.shape[0] == 0:\n","            raise FileNotFoundError('The controller was created without test splitting')\n","        return self.filepaths[self.test_index]\n","    \n","    def get_fold_paths (self, fold:int):\n","        train_indexes, val_indexes = self.get_fold_indexes(fold)\n","        train_paths = self.filepaths[train_indexes]\n","        val_paths = self.filepaths[val_indexes]\n","        return (train_paths, val_paths)\n","\n","    \n","    ############### Save images ###############\n","\n","    def save_transformed_images (self, indexes=[0], folder_path=\"\", base_name=\"\", extension=\".png\", verbose:bool=False):\n","        for index in indexes:\n","            if not os.path.exists(folder_path):\n","                os.makedirs(folder_path)\n","            filename, _ = os.path.splitext(os.path.split(self.filepaths[index])[-1])\n","            new_file_path = os.path.join(folder_path, base_name + filename + extension)\n","            \n","            if not self.lazy:\n","                img = self.data[index]\n","            else:\n","                img = np.asarray(self.transformations(Image.open(self.data[index])))\n","                \n","            plt.imshow(img.transpose((1, 2, 0)))\n","            if verbose:\n","                print (\"saving image to to \" + new_file_path)\n","            plt.savefig(new_file_path, bbox_inches='tight')\n","            if verbose:\n","                print (\"saved plot to \" + new_file_path)\n","            plt.close()\n","\n","\n","class BreakHisDataContainer(ImageFolderDataContainer):\n","    \"\"\"Class for loading the BreakHis dataset from a Path. Extends ImageFolderDataContainer.\n","    The labels are extracted from the image path with 'benign' = 0  and 'malignant' = 1\n","    \"\"\"\n","    pathLabelsList = ['benign', 'malignant']\n","    extension = \".png\"\n","\n","    def __init__(self, \n","                 folderPath:str,\n","                 lazy:bool=False, \n","                 magnification:str=\"400X\", \n","                 split_test:bool=True, \n","                 test_size:float=0.1, \n","                 split_final_val:bool=True, \n","                 final_val_size:float=0.1, \n","                 split_kfold:bool=True, \n","                 k_splits:int=5, \n","                 random_state:int=None, \n","                 transformations:torchvision.transforms=None):\n","        self.magnification=magnification\n","        super().__init__(folderPath=folderPath,\n","                         extension=self.extension,\n","                         pathLabelsList=self.pathLabelsList,\n","                         lazy=lazy,\n","                         split_test=split_test, \n","                         test_size=test_size, \n","                         split_final_val=split_final_val, \n","                         final_val_size=final_val_size, \n","                         split_kfold=split_kfold, \n","                         k_splits=k_splits, \n","                         random_state=random_state, \n","                         transformations=transformations)\n","    \n","    def extract_data (self, folderPath:str, extension:str, pathLabelsList: list[str]) -> tuple[npt.NDArray, npt.NDArray]:  \n","        # Take the paths of all images for that magnification in the given folder and subfolders and short them alphabetically\n","        filepaths = glob.glob(folderPath + \"/**/*\" + extension, recursive=True)\n","        #print(filepaths)\n","        if self.magnification != \"ALL\":\n","            filepaths = [fpath for fpath in filepaths if self.magnification in fpath]\n","        filepaths = sorted(filepaths, key=lambda s: os.path.split(s)[-1])\n","\n","        # Read all images and extract their labels from the path\n","        images = filepaths\n","        if not self.lazy:\n","            images = [np.asarray(self.transformations(Image.open(fpath))) for fpath in images]\n","        labels=[]\n","        possibleLabelsLength = len(pathLabelsList)\n","        for fpath in filepaths:\n","            for i in range(0, possibleLabelsLength):\n","                if pathLabelsList[i] in fpath:\n","                    labels.append(i)\n","                    break\n","\n","        # Store filepaths, and return images and labels\n","        self.filepaths = np.asarray(filepaths)\n","        data = np.asarray(images)\n","        labels = np.asarray(labels)\n","\n","        return data, labels\n","\n","\n","class BreakHisDataContainerPatientMulticlass(BreakHisDataContainer):\n","    \"\"\"Class for loading the BreakHis dataset from a Path. Extends BreakHisDataContainer.\n","    The labels are extracted from the extraction ID\n","    \"\"\"\n","    def extract_data (self, folderPath:str, extension:str, pathLabelsList: list[str]) -> tuple[npt.NDArray, npt.NDArray]:     \n","        # Take the paths of all images for that magnification in the given folder and subfolders and short them alphabetically\n","        filepaths = glob.glob(folderPath + \"/**/*\" + extension, recursive=True)\n","        if self.magnification != \"ALL\":\n","            filepaths = [fpath for fpath in filepaths if self.magnification in fpath]\n","        filepaths = sorted(filepaths, key=lambda s: os.path.split(s)[-1])\n","\n","        # Read all images and extract their labels from the path\n","        images = filepaths\n","        if not self.lazy:\n","            images = [np.asarray(self.transformations(Image.open(fpath))) for fpath in images]\n","        slide_ids = [(fpath.split(\"/\")[-3]).split(\"-\")[-1] for fpath in self.filepaths]\n","        unique_ids = list(set(slide_ids))\n","        labels = [unique_ids.index(id) for id in slide_ids]\n","\n","        # Store filepaths, and return images and labels\n","        self.filepaths = np.asarray(filepaths)\n","        data = np.asarray(images)\n","        labels = np.asarray(labels)\n","\n","        return data, labels\n","\n","class AntsAndBeesDataContainer(ImageFolderDataContainer):\n","    \"\"\"Image Folder Data Container for Hymenoptera dataset. Extends ImageFolderDataContainer.\n","    The labels are extracted from the image path with 'ants' = 0  and 'bees' = 1\n","    \"\"\"\n","    pathLabelsList = ['ants', 'bees']\n","    extension=\".jpg\"\n","    def __init__(self, \n","                 folderPath:str, \n","                 lazy:bool=False, \n","                 split_test:bool=True, \n","                 test_size=0.1, \n","                 split_final_val:bool=True, \n","                 final_val_size=0.1, \n","                 split_kfold:bool=True, \n","                 k_splits:int=5, \n","                 random_state:int=None, \n","                 transformations:torchvision.transforms=None):\n","\n","        super().__init__(folderPath=folderPath,\n","                         extension=self.extension,\n","                         pathLabelsList=self.pathLabelsList,\n","                         lazy=lazy,\n","                         split_test=split_test, \n","                         test_size=test_size, \n","                         split_final_val=split_final_val, \n","                         final_val_size=final_val_size, \n","                         split_kfold=split_kfold, \n","                         k_splits=k_splits, \n","                         random_state=random_state, \n","                         transformations=transformations)\n","\n","\n","class ToxoDataContainer(ImageFolderDataContainer):\n","    pathLabelsList = ['unhealthy', 'healthy']\n","    extension=\".jpg\"\n","    def __init__(self, \n","                 folderPath:str, \n","                 lazy:bool=False, \n","                 split_test:bool=True, \n","                 test_size=0.1, \n","                 split_final_val:bool=True, \n","                 final_val_size=0.1, \n","                 split_kfold:bool=True, \n","                 k_splits:int=5, \n","                 random_state:int=None, \n","                 transformations:torchvision.transforms=None):\n","\n","        super().__init__(folderPath=folderPath,\n","                         extension=self.extension,\n","                         pathLabelsList=self.pathLabelsList,\n","                         lazy=lazy,\n","                         split_test=split_test, \n","                         test_size=test_size, \n","                         split_final_val=split_final_val, \n","                         final_val_size=final_val_size, \n","                         split_kfold=split_kfold, \n","                         k_splits=k_splits, \n","                         random_state=random_state, \n","                         transformations=transformations)\n","    "],"metadata":{"id":"LGTLPx_1apxx","executionInfo":{"status":"ok","timestamp":1685908100671,"user_tz":240,"elapsed":252,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#################################################################################\n","#   The data has to have the folder structure of\n","#\n","#       unhealthy\n","#           |\n","#           |--- toxo\n","#           |--- retinoplasmosys\n","#           |--- ....\n","#           |--- ....\n","#           |--- ....\n","#       healthy\n","#           |--- normal\n","#\n","# The resulting csv will have the structure: imageName, binaryLabel, MulticlassLabel, features*\n","#################################################################################\n","\n","def get_activation(observers, observer_name):\n","    def hook(model, input, output):\n","        observers[observer_name] = output.detach()\n","    return hook\n","\n","def transform_images_to_features_csv (model, observers, observer_name, dataContainer, output_file='features.csv', sublabels=False, separator=','):\n","\n","    # Get Input\n","    data_dataloader = dataContainer.get_data_dataloader(batch_size=1,shuffle=False)\n","    data_paths = dataContainer.get_data_paths()\n","\n","    # Write file\n","    f = open(output_file, \"w\")\n","    for i, (input, label) in enumerate(data_dataloader):\n","\n","        # Obtain full mopdel output\n","        output = model(input)\n","        features = (torch.flatten(observers[observer_name])).cpu().numpy()\n","        imagePath = data_paths[i]\n","        imageName = os.path.split(imagePath)[-1]\n","        sublabel_name=''\n","        if sublabels:\n","            sublabel_name = os.path.split(os.path.split(imagePath)[-2])[-1]\n","        label_name = dataContainer.pathLabelsList[label]\n","\n","        # Convert the features in a csv string\n","        features_string = \"\"\n","        for feature in features:\n","            features_string += str(feature) + separator\n","        # for the loop construction, there is going an extra separator at the end\n","        features_string = features_string[:-1]\n","\n","        csv_line = imageName + separator + label_name + separator + sublabel_name\n","        csv_line = csv_line + separator + features_string\n","        csv_line = csv_line + \"\\n\"\n","        #print (csv_line)\n","        f.write(csv_line)\n","\n","    #close the file\n","    f.close()\n","\n","\n","def features_csv_merge (input1_csv, input2_csv, output_csv, delimiter=','):\n","    ids=[]\n","    data = {}\n","    with open(input1_csv) as csv_file:\n","        csv_reader = csv.reader(csv_file, delimiter=delimiter)\n","        line_count = 0\n","        for row in csv_reader:\n","            ids.append(row[0])\n","            data[row[0]] = [row[1], row[2], row[3:len(row)]]\n","    \n","    with open(input2_csv) as csv_file:\n","        csv_reader = csv.reader(csv_file, delimiter=delimiter)\n","        line_count = 0\n","        for row in csv_reader:\n","            [label, sublabel, f1] = data[row[0]]\n","            data[row[0]] = [label, sublabel, f1, row[3:len(row)]]\n","\n","    f = open(output_csv, \"w\")\n","    for id in ids:\n","\n","        label, sublabel, f1, f2 = data[id]\n","\n","        # Convert the features in a csv string\n","        features_string = \"\"\n","        for feature in f1:\n","            features_string += feature + delimiter\n","        for feature in f2:\n","            features_string += feature + delimiter\n","        # for the loop construction, there is going an extra delimiter at the end\n","        features_string = features_string[:-1]\n","\n","        csv_line = id + delimiter + label + delimiter + sublabel\n","        csv_line = csv_line + delimiter + features_string\n","        csv_line = csv_line + \"\\n\"\n","        #print (csv_line)\n","        f.write(csv_line)\n","\n","    #close the file\n","    f.close()\n","\n","\n","def features_csv_to_tensor (input_csv, delimiter=','):\n","    ids=[]\n","    data=[]\n","    with open(input_csv) as csv_file:\n","        csv_reader = csv.reader(csv_file, delimiter=delimiter)\n","        line_count = 0\n","        for row in csv_reader:\n","            ids.append(row[0])\n","            if (data == []):\n","                data = [np.array(row)]\n","            else:\n","                data = np.append(c, [row], axis=0)\n","\n"],"metadata":{"id":"NPktcVLYas0L","executionInfo":{"status":"ok","timestamp":1685908175267,"user_tz":240,"elapsed":161,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["crop_size = 460\n","input_shape = 224\n","data_transforms =  transforms.Compose([\n","        #transforms.CenterCrop(crop_size),\n","        transforms.Resize((input_shape, input_shape)),\n","        transforms.ToTensor()\n","    ])\n","dataContainer = ToxoDataContainer(toxoFolderPath, transformations=data_transforms, lazy=True)\n","print(dataContainer.check_intersection())\n","print(dataContainer.check_folds())\n","x, y = dataContainer.get_data()\n","print(x.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pKPk_a64ayWR","executionInfo":{"status":"ok","timestamp":1685908180172,"user_tz":240,"elapsed":121,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}},"outputId":"cbb00c02-b9c0-4f1f-83c4-2c47edd36298"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","True\n","(199,)\n"]}]},{"cell_type":"code","source":["crop_size = 460\n","input_shape = 64\n","\n","data_transforms =  transforms.Compose([\n","        transforms.CenterCrop(crop_size),\n","        transforms.Resize((input_shape, input_shape)),\n","        transforms.ToTensor()\n","    ])\n","\n","def get_densenet121_model_fc_linear (freeze=True, pretrained=True, output_features=1, num_init_features: int = 64, bn_size: int = 4, drop_rate: float = 0):\n","\n","    weights = None\n","    if pretrained:\n","        weights = models.DenseNet121_Weights.IMAGENET1K_V1\n","    \n","    model = models.densenet121(weights=weights)\n","\n","    # freeze the layers\n","    if freeze:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","    # Modify the classifier head\n","    num_ftrs = model.classifier.in_features\n","    model.classifier = nn.Sequential (    \n","        nn.Linear(num_ftrs, output_features)\n","    )\n","\n","    return model"],"metadata":{"id":"jEjmh-dca0ex","executionInfo":{"status":"ok","timestamp":1685908183628,"user_tz":240,"elapsed":132,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["fileName = \"DenseNet121.csv\"\n","\n","# Model \n","model = get_densenet121_model_fc_linear()\n","model.eval()\n","\n","# Add hook (activation) to the selected layer. It needs to be a dictionary because pointers obscure wizardy\n","observers = {}\n","observer_name='features'\n","model.features.pool0.register_forward_hook(get_activation(observers, observer_name))\n","\n","# Call the function\n","transform_images_to_features_csv (model, observers, observer_name, dataContainer, output_file=fileName, sublabels=True)\n"],"metadata":{"id":"LWN3p9OAbFzg","executionInfo":{"status":"ok","timestamp":1685908284932,"user_tz":240,"elapsed":93922,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"de603b2b-edea-4539-dde3-a582ff053729"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n","100%|██████████| 30.8M/30.8M [00:00<00:00, 63.9MB/s]\n"]}]},{"cell_type":"code","source":["crop_size = 460\n","input_shape = 64\n","\n","data_transforms =  transforms.Compose([\n","        transforms.CenterCrop(crop_size),\n","        transforms.Resize((input_shape, input_shape)),\n","        transforms.ToTensor()\n","    ])\n","\n","def get_densenet161_model_fc_linear (freeze=True, pretrained=True, output_features=1, num_init_features: int = 64, bn_size: int = 4, drop_rate: float = 0):\n","\n","    weights = None\n","    if pretrained:\n","        weights = models.DenseNet161_Weights.IMAGENET1K_V1\n","    \n","    model = models.densenet161(weights=weights)\n","\n","    # freeze the layers\n","    if freeze:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","    # Modify the classifier head\n","    num_ftrs = model.classifier.in_features\n","    model.classifier = nn.Sequential (    \n","        nn.Linear(num_ftrs, output_features)\n","    )\n","\n","    return model"],"metadata":{"id":"To411Q_SbdgB","executionInfo":{"status":"ok","timestamp":1685908315656,"user_tz":240,"elapsed":131,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["fileName = \"DenseNet161.csv\"\n","\n","# Model \n","model = get_densenet161_model_fc_linear()\n","model.eval()\n","\n","# Add hook (activation) to the selected layer. It needs to be a dictionary because pointers obscure wizardy\n","observers = {}\n","observer_name='features'\n","model.features.pool0.register_forward_hook(get_activation(observers, observer_name))\n","\n","# Call the function\n","transform_images_to_features_csv (model, observers, observer_name, dataContainer, output_file=fileName, sublabels=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vXelV9xovICX","executionInfo":{"status":"ok","timestamp":1685908478097,"user_tz":240,"elapsed":160187,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}},"outputId":"621ed465-91df-414b-ad60-2f38127a1121"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n","100%|██████████| 110M/110M [00:01<00:00, 78.0MB/s]\n"]}]},{"cell_type":"code","source":["crop_size = 460\n","input_shape = 64\n","\n","data_transforms =  transforms.Compose([\n","        transforms.CenterCrop(crop_size),\n","        transforms.Resize((input_shape, input_shape)),\n","        transforms.ToTensor()\n","    ])\n","\n","def get_densenet169_model_fc_linear (freeze=True, pretrained=True, output_features=1, num_init_features: int = 64, bn_size: int = 4, drop_rate: float = 0):\n","\n","    weights = None\n","    if pretrained:\n","        weights = models.DenseNet169_Weights.IMAGENET1K_V1\n","    \n","    model = models.densenet169(weights=weights)\n","\n","    # freeze the layers\n","    if freeze:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","    # Modify the classifier head\n","    num_ftrs = model.classifier.in_features\n","    model.classifier = nn.Sequential (    \n","        nn.Linear(num_ftrs, output_features)\n","    )\n","\n","    return model"],"metadata":{"id":"E1PtGoPuvgJu","executionInfo":{"status":"ok","timestamp":1685908493141,"user_tz":240,"elapsed":155,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["fileName = \"DenseNet169.csv\"\n","\n","# Model \n","model = get_densenet169_model_fc_linear()\n","model.eval()\n","\n","# Add hook (activation) to the selected layer. It needs to be a dictionary because pointers obscure wizardy\n","observers = {}\n","observer_name='features'\n","model.features.pool0.register_forward_hook(get_activation(observers, observer_name))\n","\n","# Call the function\n","transform_images_to_features_csv (model, observers, observer_name, dataContainer, output_file=fileName, sublabels=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LOV4M9VUvimv","executionInfo":{"status":"ok","timestamp":1685908597584,"user_tz":240,"elapsed":102095,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}},"outputId":"4b276a92-da42-45a9-ebd4-4f02be605f11"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to /root/.cache/torch/hub/checkpoints/densenet169-b2777c0a.pth\n","100%|██████████| 54.7M/54.7M [00:00<00:00, 75.6MB/s]\n"]}]},{"cell_type":"code","source":["crop_size = 460\n","input_shape = 64\n","\n","data_transforms =  transforms.Compose([\n","        transforms.CenterCrop(crop_size),\n","        transforms.Resize((input_shape, input_shape)),\n","        transforms.ToTensor()\n","    ])\n","\n","def get_densenet201_model_fc_linear (freeze=True, pretrained=True, output_features=1, num_init_features: int = 64, bn_size: int = 4, drop_rate: float = 0):\n","\n","    weights = None\n","    if pretrained:\n","        weights = models.DenseNet201_Weights.IMAGENET1K_V1\n","    \n","    model = models.densenet201(weights=weights)\n","\n","    # freeze the layers\n","    if freeze:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","    # Modify the classifier head\n","    num_ftrs = model.classifier.in_features\n","    model.classifier = nn.Sequential (    \n","        nn.Linear(num_ftrs, output_features)\n","    )\n","\n","    return model"],"metadata":{"id":"ZYxrzfVpvrDY","executionInfo":{"status":"ok","timestamp":1685908605383,"user_tz":240,"elapsed":116,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["fileName = \"DenseNet201.csv\"\n","\n","# Model \n","model = get_densenet201_model_fc_linear()\n","model.eval()\n","\n","# Add hook (activation) to the selected layer. It needs to be a dictionary because pointers obscure wizardy\n","observers = {}\n","observer_name='features'\n","model.features.pool0.register_forward_hook(get_activation(observers, observer_name))\n","\n","# Call the function\n","transform_images_to_features_csv (model, observers, observer_name, dataContainer, output_file=fileName, sublabels=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Sm2rSZyvr3e","executionInfo":{"status":"ok","timestamp":1685908719611,"user_tz":240,"elapsed":111749,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}},"outputId":"7e38fda1-52c6-4dff-cda7-1ad68da27975"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n","100%|██████████| 77.4M/77.4M [00:01<00:00, 75.4MB/s]\n"]}]},{"cell_type":"code","source":["files = ['DenseNet121.csv','DenseNet161.csv','DenseNet169.csv','Densenet201']\n","df = pd.DataFrame()\n","for file in files:\n","  data = pd.read_csv(file)\n","  df = pd.concat([df,data], axis = 1)\n","  df.to_csv('ConcatenatedFeatures.csv', index = False)"],"metadata":{"id":"Q2oxMHJJvza3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Two-File Merge\n","filename1 = \"DenseNet121.csv\"\n","filename2 = \"DenseNet201.csv\"\n","\n","output_filename = \"DenseNet121+201.csv\"\n","\n","features_csv_merge(filename1, filename1, output_filename)"],"metadata":{"id":"OBJA15vWyHjC","executionInfo":{"status":"ok","timestamp":1685909445014,"user_tz":240,"elapsed":51214,"user":{"displayName":"Cole Foster","userId":"11011135461645852245"}}},"execution_count":22,"outputs":[]}]}