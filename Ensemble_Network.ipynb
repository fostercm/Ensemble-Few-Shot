{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and processing the data into a dataloader\n",
    "re_size = 35\n",
    "crop_size = 32\n",
    "data_transforms =  transforms.Compose([\n",
    "        transforms.Resize([re_size,]),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "imageData = torchvision.datasets.ImageFolder(root = '/home/cmf290/dr/data/unhealthy',transform=data_transforms)\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(imageData,[0.6,0.2,0.2])\n",
    "dataLoader_train = DataLoader(train_data, batch_size=64,shuffle=True)\n",
    "dataLoader_val = DataLoader(val_data, batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\resnet34-b627a593.pth\n",
      "100%|██████████| 83.3M/83.3M [00:10<00:00, 8.07MB/s]\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:13<00:00, 7.42MB/s]\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\resnet101-63fe2227.pth\n",
      "100%|██████████| 171M/171M [00:25<00:00, 6.97MB/s] \n",
      "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\resnet152-394f9c45.pth\n",
      "100%|██████████| 230M/230M [00:35<00:00, 6.76MB/s] \n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\densenet121-a639ec97.pth\n",
      "100%|██████████| 30.8M/30.8M [00:06<00:00, 5.29MB/s]\n",
      "Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\densenet161-8d451a50.pth\n",
      "100%|██████████| 110M/110M [00:16<00:00, 7.19MB/s] \n",
      "Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\densenet169-b2777c0a.pth\n",
      "100%|██████████| 54.7M/54.7M [00:10<00:00, 5.29MB/s]\n",
      "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\densenet201-c1103571.pth\n",
      "100%|██████████| 77.4M/77.4M [00:11<00:00, 6.82MB/s]\n",
      "Downloading: \"https://download.pytorch.org/models/vgg11-8a719046.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\vgg11-8a719046.pth\n",
      "100%|██████████| 507M/507M [01:06<00:00, 8.02MB/s] \n",
      "Downloading: \"https://download.pytorch.org/models/vgg13-19584684.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\vgg13-19584684.pth\n",
      "100%|██████████| 508M/508M [01:13<00:00, 7.27MB/s] \n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [01:15<00:00, 7.32MB/s] \n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [01:16<00:00, 7.49MB/s] \n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to C:\\Users\\colef/.cache\\torch\\hub\\checkpoints\\inception_v3_google-0cc3c7bd.pth\n",
      "100%|██████████| 104M/104M [00:13<00:00, 7.79MB/s] \n"
     ]
    }
   ],
   "source": [
    "#Instantiate feature extractors and freeze weights\n",
    "resnet18 = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "resnet34 = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "for param in resnet34.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "resnet50 = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "resnet101 = torchvision.models.resnet101(weights='IMAGENET1K_V1')\n",
    "for param in resnet101.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "resnet152 = torchvision.models.resnet152(weights='IMAGENET1K_V1')\n",
    "for param in resnet152.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "densenet121 = torchvision.models.densenet121(weights='IMAGENET1K_V1')\n",
    "for param in densenet121.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "densenet161 = torchvision.models.densenet161(weights='IMAGENET1K_V1')\n",
    "for param in densenet161.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "densenet169 = torchvision.models.densenet169(weights='IMAGENET1K_V1')\n",
    "for param in densenet169.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "densenet201 = torchvision.models.densenet201(weights='IMAGENET1K_V1')\n",
    "for param in densenet201.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg11 = torchvision.models.vgg11(weights='IMAGENET1K_V1')\n",
    "for param in vgg11.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg13 = torchvision.models.vgg13(weights='IMAGENET1K_V1')\n",
    "for param in vgg13.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg19 = torchvision.models.vgg19(weights='IMAGENET1K_V1')\n",
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "inception_v3 = torchvision.models.inception_v3(weights='IMAGENET1K_V1')\n",
    "for param in inception_v3.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of names for feature extractor models used instantiated in the above cell\n",
    "models = list([resnet18,resnet34,resnet50,resnet101,resnet152,densenet121,densenet161,densenet169,densenet201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extractor, concatenation, and fully connected neural network with one hidden layer\n",
    "class Feature_Extractor_ClassifierNetwork(nn.Module):\n",
    "    def __init__(self, models, hidden_size, num_classes):\n",
    "        super(Feature_Extractor_ClassifierNetwork, self).__init__()\n",
    "\n",
    "        assert type(models) == list and (not len(models) == 0)\n",
    "\n",
    "        self.feature_models = []\n",
    "        self.input_filters = 0\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #loop through the models in the model list defined in above cell\n",
    "        for i, model in enumerate(models):\n",
    "          #define layers of each feature extrator model using list(model.childen())\n",
    "          layers = list(model.children())\n",
    "          #count the number of features extracted right before the classification layer *hint: layers[-1].in_features\n",
    "          feature_count = layers[-1].in_features\n",
    "          #add the number of output features together for everymodel and store in input_filters\n",
    "          self.input_filters += feature_count\n",
    "          #append the feature extractor model (without the classifier head to feature_models hint *nn.Sequential(*list(model.children())[:-1]))\n",
    "          self.feature_models.append(nn.Sequential(*list(model.children())[:-1]))\n",
    "\n",
    "        # define first fully connected layer with input as input_filters and output as hidden_size)\n",
    "        self.first_layer = nn.Linear(self.input_filters,hidden_size)\n",
    "        # define hyperbolic tangent activation\n",
    "        self.hyperbolic_tan = nn.Tanh()\n",
    "        # define second fully connected layer with hidden size input and number of classes as output\n",
    "        self.second_layer = nn.Linear(hidden_size,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      # define an empty tensor to store the concatenated features\n",
    "      concatenated_features = torch.Tensor()\n",
    "      # loop through each feature model in feature_model define above\n",
    "      for i, model in enumerate(self.feature_models):\n",
    "        # feed your input images, x, through the feature extractors then concatenate output of all the models into feat_concat\n",
    "        output = model(x)\n",
    "        output = output[:,:,0,0]\n",
    "        # *warning: concatenate horizontally be mindful of axis of concatenation\n",
    "        concatenated_features = torch.cat((concatenated_features,output),dim=1)\n",
    "\n",
    "      # feed concatenated features into first  fully connect layer\n",
    "      output = self.first_layer(concatenated_features)\n",
    "      # feed output from first  fully connect layer into hyp tan activation\n",
    "      output = self.hyperbolic_tan(output)\n",
    "      # feed output of the activation into second fully connect layer\n",
    "      output = self.second_layer(output)\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0555,  0.5938, -0.1299, -0.2065, -0.0646],\n",
       "        [ 0.0268, -0.1605, -0.1201, -0.1783, -0.0557],\n",
       "        [ 0.1819,  0.0191, -0.2334, -0.2857,  0.0391],\n",
       "        [-0.1429,  0.2425, -0.0398, -0.2031, -0.0804]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check model runs with a random tensor\n",
    "test_model = Feature_Extractor_ClassifierNetwork(models,1024,5)\n",
    "x=torch.randn(4,3,32,32)\n",
    "test_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on your optimization/training loop designed previously with cross-entropy loss\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, criterion):\n",
    "    transformation = transforms.ToTensor()\n",
    "    #set model to train mode\n",
    "    model.train()\n",
    "    #initialize training varaibles to track, i.e. loss and accuracy\n",
    "    running_loss = 0.\n",
    "    running_accuracy = 0.\n",
    "    #enumeration loop\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # X, Y = X.to(device), Y.to(device)\n",
    "        inputs, labels = data\n",
    "        new_labels = []\n",
    "        for idx, label in enumerate(labels):\n",
    "            adjusted_label = [0.,0.,0.,0.,0.]\n",
    "            adjusted_label[label] = 1.\n",
    "            adjusted_label = np.float32(np.array(adjusted_label))\n",
    "            new_labels.append(adjusted_label)\n",
    "        labels = new_labels\n",
    "        labels = transformation(np.array(labels))\n",
    "        labels = np.transpose(labels,[1,-1,0])\n",
    "        labels = labels[:,:,0]\n",
    "        #clear the grad of each parameter\n",
    "        optimizer.zero_grad()\n",
    "        #forward pass\n",
    "        outputs = model(inputs)\n",
    "        #compute loss use functional loss: nnF.mse_loss\n",
    "        loss = criterion(outputs,labels)\n",
    "        #backward pass\n",
    "        loss.backward()\n",
    "        #update parameters with optimizer\n",
    "        optimizer.step()\n",
    "        #record the loss with the loss variable\n",
    "        running_loss += loss.item()\n",
    "        #convert prediction to int for accuracy computation.\n",
    "        for idx, out_data in enumerate(outputs):\n",
    "            max_idx = torch.argmax(out_data)\n",
    "            outputs[idx] = torch.tensor([0.,0.,0.,0.,0.])\n",
    "            outputs[idx][max_idx] = 1.\n",
    "        # #measure accuracy on this batch and sum to accuracy variable\n",
    "\n",
    "        correct = (outputs == labels).float().sum() / 5\n",
    "\n",
    "\n",
    "        running_accuracy += correct\n",
    "\n",
    "    val_running_loss = 0\n",
    "    val_running_accuracy = 0\n",
    "\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        # X, Y = X.to(device), Y.to(device)\n",
    "        inputs, labels = data\n",
    "        new_labels = []\n",
    "        for idx, label in enumerate(labels):\n",
    "            adjusted_label = [0.,0.,0.,0.,0.]\n",
    "            adjusted_label[label] = 1.\n",
    "            adjusted_label = np.float32(np.array(adjusted_label))\n",
    "            new_labels.append(adjusted_label)\n",
    "        labels = new_labels\n",
    "        labels = transformation(np.array(labels))\n",
    "        labels = np.transpose(labels,[1,-1,0])\n",
    "        labels = labels[:,:,0]\n",
    "        #clear the grad of each parameter\n",
    "        optimizer.zero_grad()\n",
    "        #forward pass\n",
    "        outputs = model(inputs)\n",
    "        #compute loss use functional loss: nnF.mse_loss\n",
    "        loss = criterion(outputs,labels)\n",
    "        #record the loss with the loss variable\n",
    "        val_running_loss += loss.item()\n",
    "        #convert prediction to int for accuracy computation.\n",
    "        for idx, out_data in enumerate(outputs):\n",
    "            max_idx = torch.argmax(out_data)\n",
    "            outputs[idx] = torch.tensor([0.,0.,0.,0.,0.])\n",
    "            outputs[idx][max_idx] = 1.\n",
    "        # #measure accuracy on this batch and sum to accuracy variable\n",
    "\n",
    "        correct = (outputs == labels).float().sum() / 5\n",
    "\n",
    "\n",
    "        val_running_accuracy += correct\n",
    "\n",
    "\n",
    "    #compute average loss and accuracy\n",
    "    loss_train = running_loss / len(train_dataloader)\n",
    "    accuracy_train = running_accuracy / len(train_dataloader.dataset)\n",
    "    loss_val = val_running_loss / len(val_dataloader)\n",
    "    accuracy_val = val_running_accuracy / len(val_dataloader.dataset)\n",
    "    #return accuracy and loss\n",
    "    return loss_train, accuracy_train, loss_val, accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate model, optimizer, and criterion and train\n",
    "model = Feature_Extractor_ClassifierNetwork(models,1024,5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_train_list=[]\n",
    "acc_train_list=[]\n",
    "loss_val_list = []\n",
    "acc_val_list = []\n",
    "\n",
    "for epoch in range(0, 3):\n",
    "    #-------- perform training --------------------------------\n",
    "    loss_train, acc_train, loss_val, acc_val = train(model,optimizer,dataLoader_train,dataLoader_val,criterion)\n",
    "    loss_train_list.append(loss_train)\n",
    "    acc_train_list.append(acc_train)\n",
    "    loss_val_list.append(loss_val)\n",
    "    acc_val_list.append(acc_val)\n",
    "\n",
    "    print('epoch', epoch, 'training loss:', loss_train, 'acc:', acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the loss and accuracy results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "ax[0].plot(np.arange(0,len(loss_train_list)), loss_train_list, '-b', label='training loss')\n",
    "ax[0].plot(np.arange(0,len(loss_val_list)), loss_val_list, '-g', label='validation loss')\n",
    "ax[0].set_xlabel('epoch',fontsize=16)\n",
    "ax[0].legend(fontsize=16)\n",
    "ax[0].grid(True)\n",
    "ax[1].plot(np.arange(0,len(acc_train_list)), acc_train_list, '-b', label='training accuracy')\n",
    "ax[1].plot(np.arange(0,len(acc_val_list)), acc_val_list, '-g', label='validation accuracy')\n",
    "ax[1].set_xlabel('epoch',fontsize=16)\n",
    "ax[1].legend(fontsize=16)\n",
    "ax[1].grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
